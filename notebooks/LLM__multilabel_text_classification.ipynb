{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8d81cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1de240c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "model_checkpoint = 'distilbert-base-uncased'\n",
    "dataset_name = \"sem_eval_2018_task_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa47d3cd",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed52113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset = load_dataset(dataset_name, \"subtask5.english\")\n",
    "\n",
    "\n",
    "# get mapping of labels\n",
    "labels = [label for label in dataset['train'].features.keys() if label not in ['ID', 'Tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85bcdf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28c248f",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "303c4d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def preprocess_data(examples):\n",
    "  # take a batch of texts\n",
    "  text = examples[\"Tweet\"]\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "  # add labels\n",
    "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  labels_matrix = np.zeros((len(text), len(labels)))\n",
    "  # fill numpy array\n",
    "  for idx, label in enumerate(labels):\n",
    "    labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "  encoding[\"labels\"] = labels_matrix.tolist()\n",
    "  \n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eff2347f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/Amiros/.cache/huggingface/datasets/sem_eval_2018_task_1/subtask5.english/1.1.0/a7c0de8b805f1988b118882fb289ccfbbeb9085c7820b6f046b5887e234af182/cache-308da669712af75e.arrow\n",
      "Loading cached processed dataset at /Users/Amiros/.cache/huggingface/datasets/sem_eval_2018_task_1/subtask5.english/1.1.0/a7c0de8b805f1988b118882fb289ccfbbeb9085c7820b6f046b5887e234af182/cache-e56b686f5b41d255.arrow\n",
      "Loading cached processed dataset at /Users/Amiros/.cache/huggingface/datasets/sem_eval_2018_task_1/subtask5.english/1.1.0/a7c0de8b805f1988b118882fb289ccfbbeb9085c7820b6f046b5887e234af182/cache-f412a5b21a4d7c16.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset_tokenized = dataset.map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07608b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 6838\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3259\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 886\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set format for pytorch\n",
    "dataset_tokenized.set_format(\"torch\")\n",
    "\n",
    "dataset_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928a0c72",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2807f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (AutoModelForSequenceClassification,\n",
    "                          Trainer, \n",
    "                          TrainingArguments\n",
    "                         )\n",
    "from transformers import EvalPrediction\n",
    "from sklearn.metrics import  f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85b31965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the hyperparamteres relying on the suggested values from https://arxiv.org/abs/1905.05583\n",
    "epochs = 5\n",
    "train_batch_size = 16\n",
    "eval_batch_size = 16\n",
    "warmup_steps = 500\n",
    "learning_rate = 5e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adbdd82",
   "metadata": {},
   "source": [
    "We have to set the problem_type to be `multi_label_classification`. This will make sure the appropriate loss [`BCEWithLogitsLoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) function is used. BCEWithLogitLoss uses multiple Sigmoid (equal to the number of labels) when computing loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e45c8c5",
   "metadata": {},
   "source": [
    "## Get the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf52917",
   "metadata": {},
   "source": [
    "multi_label_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cffb72cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# download model from model hub\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    problem_type=\"multi_label_classification\", \n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be01f3",
   "metadata": {},
   "source": [
    "## Fine-tuning configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d6cbc",
   "metadata": {},
   "source": [
    "I will freeze the DistilBERT model layers except for the dense top layer, as the first layers of encode universal features while the top layers are more task-specific. This will also help with trianing speed and reduces memory usage during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a059095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_classifier.weight\n",
      "pre_classifier.bias\n",
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "# Freeze all base-model layers in order to speed up the fine-tuning process\n",
    "for param in model.distilbert.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# check which layers require gradiantes (i.e trainable)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        print(name) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7238f9f0",
   "metadata": {},
   "source": [
    "## Define evaluation metric\n",
    "\n",
    "\n",
    "Since we are dealing with imbalance data, I will use a `weighted-averaged F1` score. The weighted-averaged F1 score is calculated by taking the mean of all per-class F1 scores while considering each classâ€™s support (support is the number of actual occurrences of the class in the dataset).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ee86da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics function for multi-label classification\n",
    "def multi_label_metrics(predictions, labels=labels, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Multi-label classification metric for Trainer\n",
    "    \n",
    "    The steps taken in this function includes:\n",
    "        1. Tokenization with padding and truncation provided by the max_lentgh\n",
    "        2. Encode labels as a vector\n",
    "        \n",
    "    Keyword arguments:\n",
    "    predictions -- Logits from model\n",
    "    labels -- name of label columns (List)\n",
    "    threshold -- class threshold (int) default value is 0.5\n",
    "    \"\"\"\n",
    "    \n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_average = f1_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "\n",
    "    # return as dictionary\n",
    "    metrics = {'f1_weighted': f1_average\n",
    "              }\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e891c",
   "metadata": {},
   "source": [
    "## Define loss function\n",
    "\n",
    "Since we are dealing with imbalanced data, I will update the default loss function to take `pos_weight` for labels as recommedned by the [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss) for `BCEWithLogitsLoss` as the ratio between the negative counts and the positive counts for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c62314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pos_weight(training_data):\n",
    "    \"\"\"\n",
    "    Function to calculate pos_weight for labels\n",
    "    \"\"\"\n",
    "    num_positives = torch.sum(training_data['labels'], dim=0)\n",
    "    num_negatives = len(training_data['labels']) - num_positives\n",
    "    pos_weight  = num_negatives / num_positives\n",
    "    \n",
    "    return pos_weight\n",
    "\n",
    "pos_weight = calculate_pos_weight(dataset_tokenized['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dcc0e6",
   "metadata": {},
   "source": [
    "## Model for multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "911c0b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilabelTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), \n",
    "                        labels.float().view(-1, self.model.config.num_labels))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5506ab72",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef4e0efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../output/',\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    warmup_steps=warmup_steps,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=f\"../output/logs\",\n",
    "    logging_steps=2,\n",
    "    learning_rate=float(learning_rate),\n",
    "    metric_for_best_model=\"f1_weighted\",\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c0bc711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Trainer instance\n",
    "trainer = MultilabelTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset_tokenized['train'],\n",
    "    eval_dataset=dataset_tokenized['validation'],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06bf6856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 6838\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2140\n",
      "  Number of trainable parameters = 599051\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2140' max='2140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2140/2140 51:27, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.003900</td>\n",
       "      <td>1.043456</td>\n",
       "      <td>0.561322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.209200</td>\n",
       "      <td>0.966692</td>\n",
       "      <td>0.589762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.933600</td>\n",
       "      <td>0.930037</td>\n",
       "      <td>0.594589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.003800</td>\n",
       "      <td>0.905994</td>\n",
       "      <td>0.597023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.793900</td>\n",
       "      <td>0.902524</td>\n",
       "      <td>0.600004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 886\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../output/checkpoint-428\n",
      "Configuration saved in ../output/checkpoint-428/config.json\n",
      "Model weights saved in ../output/checkpoint-428/pytorch_model.bin\n",
      "tokenizer config file saved in ../output/checkpoint-428/tokenizer_config.json\n",
      "Special tokens file saved in ../output/checkpoint-428/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 886\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../output/checkpoint-856\n",
      "Configuration saved in ../output/checkpoint-856/config.json\n",
      "Model weights saved in ../output/checkpoint-856/pytorch_model.bin\n",
      "tokenizer config file saved in ../output/checkpoint-856/tokenizer_config.json\n",
      "Special tokens file saved in ../output/checkpoint-856/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 886\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../output/checkpoint-1284\n",
      "Configuration saved in ../output/checkpoint-1284/config.json\n",
      "Model weights saved in ../output/checkpoint-1284/pytorch_model.bin\n",
      "tokenizer config file saved in ../output/checkpoint-1284/tokenizer_config.json\n",
      "Special tokens file saved in ../output/checkpoint-1284/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 886\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../output/checkpoint-1712\n",
      "Configuration saved in ../output/checkpoint-1712/config.json\n",
      "Model weights saved in ../output/checkpoint-1712/pytorch_model.bin\n",
      "tokenizer config file saved in ../output/checkpoint-1712/tokenizer_config.json\n",
      "Special tokens file saved in ../output/checkpoint-1712/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 886\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../output/checkpoint-2140\n",
      "Configuration saved in ../output/checkpoint-2140/config.json\n",
      "Model weights saved in ../output/checkpoint-2140/pytorch_model.bin\n",
      "tokenizer config file saved in ../output/checkpoint-2140/tokenizer_config.json\n",
      "Special tokens file saved in ../output/checkpoint-2140/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../output/checkpoint-2140 (score: 0.6000038161310423).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2140, training_loss=0.977554079361051, metrics={'train_runtime': 3089.0824, 'train_samples_per_second': 11.068, 'train_steps_per_second': 0.693, 'total_flos': 1132446821076480.0, 'train_loss': 0.977554079361051, 'epoch': 5.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb086437",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62874b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8545651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.920563280582428,\n",
       " 'eval_f1_weighted': 0.5942335301853315,\n",
       " 'eval_runtime': 233.5157,\n",
       " 'eval_samples_per_second': 13.956,\n",
       " 'eval_steps_per_second': 0.874,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result = trainer.evaluate(eval_dataset=dataset_tokenized['test'])\n",
    "\n",
    "# save the evaluation result\n",
    "hyperparams = {\"model\": model_checkpoint, \"evaluation\": eval_result}\n",
    "e = evaluate.save(\"../output/experiments/\", **hyperparams)\n",
    "\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8e2033",
   "metadata": {},
   "source": [
    "## Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a8ff01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 3259\n",
      "  Batch size = 16\n"
     ]
    }
   ],
   "source": [
    "# get last hidden state of the trained model\n",
    "outputs = trainer.predict(dataset_tokenized['test'])\n",
    "logits = outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941373b",
   "metadata": {},
   "source": [
    "Now I'll get the logits from the model output and apply `sigmoid` with threshold of 0.5 to predict which emotions are present.\n",
    "\n",
    "The reason I am using sigmoid here is that a key property of sigmoid is that the probabilities produced by a sigmoid are independent, and therefore are not constrained to sum to one.\n",
    "\n",
    "Since in a multi-label classification setting there are more than one right answer = Non-exclusive outputs, appling a sigmoid function to each element of the raw output independently makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "241edbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotions(model_output, threshold=0.5):\n",
    "     \n",
    "    # apply sigmoid + threshold\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(model_output).squeeze().cpu())\n",
    "    # initialize an array of the same size\n",
    "    predictions = np.zeros(probs.shape)\n",
    "    # assign 1 for values where the predicted probalbity is larger than the threshold\n",
    "    predictions[np.where(probs >= threshold)] = 1\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b74d59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.65      0.74      0.70      1101\n",
      "anticipation       0.18      0.46      0.26       425\n",
      "     disgust       0.63      0.75      0.69      1099\n",
      "        fear       0.28      0.66      0.39       485\n",
      "         joy       0.75      0.81      0.78      1442\n",
      "        love       0.36      0.84      0.50       516\n",
      "    optimism       0.61      0.76      0.67      1143\n",
      "   pessimism       0.20      0.60      0.30       375\n",
      "     sadness       0.47      0.70      0.57       960\n",
      "    surprise       0.11      0.56      0.19       170\n",
      "       trust       0.09      0.56      0.16       153\n",
      "\n",
      "   micro avg       0.43      0.73      0.54      7869\n",
      "   macro avg       0.39      0.68      0.47      7869\n",
      "weighted avg       0.53      0.73      0.59      7869\n",
      " samples avg       0.43      0.72      0.51      7869\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hf/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/hf/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_emotions(logits)\n",
    "print(classification_report(outputs[1], y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0210c4",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe800c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The trained model is {round(model.get_memory_footprint()/1e6)} mb.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f0e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save locally\n",
    "model.save_pretrained(f\"../output/model/{e.name.split('.')[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8fd268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to Hub\n",
    "model.push_to_hub(f\"amir_llm_multiclass\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
